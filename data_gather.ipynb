{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import textract\n",
    "import json\n",
    "from rank_bm25 import BM25Okapi\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipedia\n",
    "import spacy\n",
    "import pdftotext\n",
    "import PyPDF2\n",
    "import re\n",
    "from pdf2image import convert_from_path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Extractor:\n",
    "    def __init__(self):\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "        self.df = pd.read_excel(\"data/grammars.xlsx\")\n",
    "        self.lang_list = [\"ca\", \"zh\", \"en\", \"fr\", \"de\", \"it\", \"pt\", \"ru\", \"es\"]\n",
    "        self.models = dict()\n",
    "        self.stopwords = dict()\n",
    "        self.embedder = SentenceTransformer('bert-base-multilingual-cased')\n",
    "\n",
    "        modelPath = \"bert-base-multilingual-cased\"\n",
    "\n",
    "        self.embedder.save(modelPath)\n",
    "        self.embedder = SentenceTransformer(modelPath)\n",
    "\n",
    "        with open(\"data/language_files.json\", 'r') as file:\n",
    "            self.language_files = json.load(file)\n",
    "\n",
    "        for lang in self.lang_list:\n",
    "            exec(\"from spacy.lang.%s.stop_words import STOP_WORDS\" % (lang))\n",
    "            exec(\"self.stopwords[lang] = spacy.lang.%s.stop_words.STOP_WORDS\" % (lang))\n",
    "            if lang in (\"en\", \"zh\"):\n",
    "                model_name = lang + \"_core_web_sm\"\n",
    "            else:\n",
    "                model_name = lang + \"_core_news_sm\"\n",
    "\n",
    "            self.models[lang] = spacy.load(model_name, disable=['parser', 'ner'])\n",
    "\n",
    "    def get_lang(self, filename):\n",
    "        text = textract.process(filename).decode(\"utf-8\") \n",
    "        lang = detect(text)\n",
    "        return lang\n",
    "\n",
    "    def first_letter(self, s):\n",
    "        m = re.search(r'[a-z]', s, re.I)\n",
    "        if m is not None:\n",
    "            return s[m.start()]\n",
    "        return \"A\"\n",
    "\n",
    "    def end_of_sentence(self, text):\n",
    "        text = text.strip(\"\\n \")\n",
    "        stop = ('...', '.', '?', '!', '!!!', '…')\n",
    "        for item in stop:\n",
    "            if text.endswith(item):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def digits(self, text):\n",
    "        num = 0\n",
    "        for i in text:\n",
    "            if i.isdigit():\n",
    "                num += 1\n",
    "        return num\n",
    "\n",
    "    def get_term(self, term, language):\n",
    "        if language == \"en\":\n",
    "            return term\n",
    "        # get languages\n",
    "        soup = BeautifulSoup(urlopen('http://en.wikipedia.org/wiki/' + (term[:1].upper() + term[1:]).replace(' ', '_')), features=\"lxml\")\n",
    "        interwikihead = soup.find('li', class_=('interlanguage-link interwiki-' + language + ' mw-list-item'))\n",
    "\n",
    "        try:\n",
    "            title = re.split(' \\(| –', interwikihead.a.get('title'))[0]\n",
    "            return title.lower()\n",
    "        except:\n",
    "            return None \n",
    "\n",
    "    def get_description(self, term, language):\n",
    "        soup = BeautifulSoup(urlopen('http://en.wikipedia.org/wiki/' + (term[:1].upper() + term[1:]).replace(' ', '_')), features=\"lxml\")\n",
    "        interwikihead = soup.find('li', class_=('interlanguage-link interwiki-' + language + ' mw-list-item'))\n",
    "\n",
    "        try:\n",
    "            if language == \"en\":\n",
    "                title = term[:1].upper() + term[1:]\n",
    "            else:\n",
    "                title = interwikihead.a.get('title').split(u' – ')[0]\n",
    "            wikipedia.set_lang(language)\n",
    "            page = wikipedia.page(title)\n",
    "            return page.summary\n",
    "        except:\n",
    "            return None \n",
    "\n",
    "    def get_new_paragraphs(self, paragraphs, page_numbers):\n",
    "        new_paragraphs = [paragraphs[0]]\n",
    "        new_page_numbers = dict()\n",
    "        new_page_numbers[paragraphs[0]] = [page_numbers[0]]\n",
    "        for i in range(1, len(paragraphs)):\n",
    "            paragraph = paragraphs[i].strip(\" \\n\")\n",
    "            if len(paragraph) > 0:\n",
    "                if (not self.end_of_sentence(new_paragraphs[-1])) | self.first_letter(paragraphs[i]).islower():\n",
    "                    index = new_page_numbers[new_paragraphs[-1]]\n",
    "                    del new_page_numbers[new_paragraphs[-1]]\n",
    "                    new_paragraphs[-1] += paragraphs[i]\n",
    "                    if index[-1] != page_numbers[i]:\n",
    "                        index.append(page_numbers[i])\n",
    "                    new_page_numbers[new_paragraphs[-1]] = index\n",
    "                else:\n",
    "                    new_paragraphs.append(paragraphs[i])\n",
    "                    new_page_numbers[paragraphs[i]] = [page_numbers[i]]\n",
    "        return new_paragraphs, new_page_numbers\n",
    "\n",
    "    def make_dir(self, fname):\n",
    "\n",
    "        directory = os.path.dirname(fname)\n",
    "        if not os.path.exists(directory):\n",
    "            os.system(\"mkdir -p \\\"{directory}\\\"\")\n",
    "        os.system(\"touch \\\"{os.path.basename(new_fname)}\\\"\")\n",
    "\n",
    "    def rerank(self, query, entries, embeddings_dict):\n",
    "\n",
    "        corpus_embeddings = self.embedder.encode(entries)\n",
    "\n",
    "        query_embedding = self.embedder.encode(query)\n",
    "\n",
    "        # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "        closest_n = 5\n",
    "        distances = scipy.spatial.distance.cdist(query_embedding, corpus_embeddings, \"cosine\")[0]\n",
    "\n",
    "        results = zip(range(len(distances)), distances)\n",
    "        results = sorted(results, key=lambda x: x[1])\n",
    "\n",
    "        ans = []\n",
    "\n",
    "        for idx, distance in results[0:closest_n]:\n",
    "            ans.append(entries[idx])\n",
    "\n",
    "        return ans\n",
    "\n",
    "    def extract(self, lang_about, query, method, description=True):\n",
    "\n",
    "        #try:\n",
    "\n",
    "            ans = []\n",
    "            image_files = []\n",
    "            fnames = []\n",
    "            i = 0\n",
    "            filenames = self.language_files[lang_about]\n",
    "            for item in filenames:\n",
    "\n",
    "                fname = item[0]\n",
    "                short_name = fname\n",
    "\n",
    "                lang = item[1]\n",
    "                absolute_path = \"data/\"\n",
    "                fname = absolute_path + fname\n",
    "                fname_par = (absolute_path + \"Grammars_Paragraphs/\" + os.path.basename(fname)).replace(\"pdf\", \"json\")\n",
    "                fname_lem = (absolute_path + \"Grammars_Lemmas/\" + os.path.basename(fname)).replace(\"pdf\", \"json\")\n",
    "                fname_num = (absolute_path + \"Grammars_Page_Numbers/\" + os.path.basename(fname)).replace(\"pdf\", \"json\")\n",
    "\n",
    "\n",
    "                nlp = self.models[lang]\n",
    "                stopwords_lang = self.stopwords[lang]\n",
    "\n",
    "                if not os.path.exists(fname_par):\n",
    "                    \n",
    "                    command = \"rclone copy \\\"gdrive:\" + short_name + \"\\\" \\\"data/\" + os.path.dirname(short_name) + \"\\\" --no-traverse --drive-chunk-size 32M -P\"\n",
    "                    print(command)\n",
    "                    os.system(command)\n",
    "\n",
    "                    with open(fname, 'rb') as f:\n",
    "                        pdf = pdftotext.PDF(f)\n",
    "                    page_numbers = []\n",
    "                    paragraphs = []\n",
    "                    lemmatized_paragraphs = []\n",
    "                    for j in range (len(pdf)):\n",
    "                        addition = re.split('  ', pdf[j])\n",
    "                        paragraphs.extend(addition)\n",
    "                        for paragraph in addition:\n",
    "                            page_numbers.append(j)\n",
    "                    new_paragraphs, new_page_numbers = self.get_new_paragraphs(paragraphs, page_numbers)\n",
    "\n",
    "                    for paragraph in new_paragraphs:\n",
    "                        lemmatized_paragraph = []\n",
    "                        doc = nlp(paragraph.lower())\n",
    "                        for token in doc:\n",
    "                            if token.lemma_ not in stopwords_lang and token.is_alpha:\n",
    "                                lemmatized_paragraph.append(token.lemma_)\n",
    "                        lemmatized_paragraphs.append(lemmatized_paragraph)\n",
    "\n",
    "                    self.make_dir(fname_lem)\n",
    "                    self.make_dir(fname_par)\n",
    "                    self.make_dir(fname_num)\n",
    "\n",
    "                    with open(fname_lem, 'w') as outfile:\n",
    "                        lem = json.dumps(lemmatized_paragraphs)\n",
    "                        outfile.write(lem)\n",
    "                    with open(fname_par, 'w') as outfile:\n",
    "                        par = json.dumps(new_paragraphs)\n",
    "                        outfile.write(par)\n",
    "                    with open(fname_num, 'w') as outfile:\n",
    "                        num = json.dumps(new_page_numbers)\n",
    "                        outfile.write(num)\n",
    "\n",
    "                else:\n",
    "                    with open(fname_par, 'r') as file:\n",
    "                        new_paragraphs = json.load(file)\n",
    "                    with open(fname_lem, 'r') as file:\n",
    "                        lemmatized_paragraphs = json.load(file)\n",
    "                    with open(fname_num, 'r') as file:\n",
    "                        new_page_numbers = json.load(file)\n",
    "\n",
    "                pdf_file = PyPDF2.PdfReader(fname)\n",
    "                bm25 = BM25Okapi(lemmatized_paragraphs)\n",
    "\n",
    "                if description:\n",
    "                    term = self.get_term(query, lang)\n",
    "                    fname_desc = \"data/Grammars_Descriptions/\" + term + \".json\"\n",
    "                    fname_desc_lem = \"data/Grammars_Descriptions/\" + term + \"_lemmatized.json\"\n",
    "\n",
    "                    if not os.path.exists(fname_desc):\n",
    "                        desc = self.get_description(query, lang)\n",
    "                        desc_nlp = nlp(self.get_description(query, lang))\n",
    "                        lemmatized_desc = []\n",
    "\n",
    "                        for token in desc_nlp:\n",
    "                            if token.lemma_ not in stopwords_lang and token.is_alpha:\n",
    "                                lemmatized_desc.append(token.lemma_)\n",
    "\n",
    "                        self.make_dir(fname_desc)\n",
    "                        self.make_dir(fname_desc_lem)\n",
    "\n",
    "                        with open(fname_desc, 'w') as outfile:\n",
    "                            file_desc = json.dumps(desc)\n",
    "                            outfile.write(file_desc)\n",
    "                        with open(fname_desc_lem, 'w') as outfile:\n",
    "                            file_desc_lem = json.dumps(lemmatized_desc)\n",
    "                            outfile.write(file_desc_lem)\n",
    "\n",
    "                    else:\n",
    "                        with open(fname_desc, 'r') as file:\n",
    "                            desc = json.load(file)\n",
    "                        with open(fname_desc_lem, 'r') as file:\n",
    "                            lemmatized_desc = json.load(file)\n",
    "\n",
    "                    query_translated = lemmatized_desc\n",
    "\n",
    "                else:\n",
    "                    term = nlp(self.get_term(query, lang))\n",
    "                    query_translated = []\n",
    "\n",
    "                    for token in term:\n",
    "                        if token.lemma_ not in stopwords_lang and token.is_alpha:\n",
    "                            query_translated.append(token.lemma_)\n",
    "\n",
    "                top_n = bm25.get_top_n(query_translated, new_paragraphs, n=5)\n",
    "\n",
    "                if method == \"BM25\":\n",
    "                    ans += top_n\n",
    "\n",
    "                else:\n",
    "                    reranked = self.rerank(query_translated, top_n, dict())\n",
    "                    ans += reranked\n",
    "\n",
    "                fnames.append(short_name)\n",
    "\n",
    "            fname_indices = []\n",
    "            \n",
    "            return ans\n",
    "\n",
    "        #except:\n",
    "            #pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список всех языков, которые есть в базе данных, файл language_files.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_files = dict()\n",
    "lang_set = set()\n",
    "\n",
    "data = pd.read_excel(\"/home/aruhaizen/project/linguistic_data_extraction/grammars_database.xlsx\")\n",
    "for index, row in data.iterrows():\n",
    "    if type(row[\"the language described\"]) != float:\n",
    "        lang_set.add(row[\"the language described\"])\n",
    "        if row[\"the language described\"] not in language_files:\n",
    "            language_files[row[\"the language described\"]] = []\n",
    "        language_files[row[\"the language described\"]].append([row[\"full path\"].replace(\"Language_Stuff\", \"Grammars\"), row[\"meta language\"]])\n",
    "\n",
    "with open(\"language_files.json\", \"w\") as outfile:\n",
    "    json.dump(language_files, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cogui', 'Lule', 'Tibetan', 'Karelian', 'Samaritan Aramaic', 'Albanian-Gheg', 'Kafa', 'Pampangan']\n"
     ]
    }
   ],
   "source": [
    "lang_list = list(lang_set)\n",
    "print(lang_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wals = pd.read_csv(\"wals_word_order.csv\")\n",
    "targets = dict(zip(wals.name, wals.description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yaqui\n",
      "Wyandot\n",
      "Udmurt\n",
      "Oksapmin\n",
      "Garo\n",
      "Hidatsa\n",
      "Salinan\n",
      "Mekens\n",
      "Yurok\n",
      "Biloxi\n",
      "Macushi\n",
      "Pipil\n",
      "Barupu\n",
      "Choctaw\n",
      "Rikbaktsa\n",
      "Basque\n",
      "Blackfoot\n",
      "Apurinã\n",
      "Thai\n",
      "Yagua\n",
      "Khmer\n",
      "Kuna\n",
      "Piro\n",
      "Gitksan\n",
      "Usarufa\n",
      "Carib\n",
      "Maricopa\n",
      "Nenets\n",
      "Haida\n",
      "Hup\n",
      "Amahuaca\n",
      "Menya\n",
      "Ayoreo\n",
      "Quileute\n",
      "Evenki\n",
      "Desano\n",
      "Dâw\n",
      "Kharia\n",
      "Hupa\n",
      "Mosetén\n",
      "Yuchi\n",
      "Galo\n",
      "Wiyot\n",
      "Jingpho\n",
      "Santa\n",
      "Sherpa\n",
      "Chrau\n",
      "Chukchi\n",
      "Burmese\n",
      "Camling\n",
      "Mohawk\n",
      "Balti\n",
      "Lisu\n",
      "Apatani\n",
      "Crow\n",
      "Passamaquoddy-Maliseet\n",
      "Estonian\n",
      "Dhimal\n",
      "Temiar\n",
      "Thulung\n",
      "Timucua\n",
      "Uyghur\n",
      "Cantonese\n",
      "Mundari\n",
      "Magar\n",
      "Mara\n",
      "Turkmen\n",
      "Zuni\n",
      "Mansi\n",
      "Ket\n",
      "Wichita\n",
      "Azerbaijani\n",
      "Cherokee\n",
      "Jamsay\n",
      "Canela\n",
      "Mizo\n",
      "Skou\n",
      "Khanty\n",
      "Pirahã\n",
      "O'odham\n",
      "Luiseño\n",
      "Burushaski\n",
      "Darma\n",
      "Pilagá\n",
      "Awa\n",
      "Seneca\n",
      "Mandarin\n",
      "Moghol\n",
      "Mutsun\n",
      "Korean\n",
      "Kutenai\n",
      "Baure\n",
      "Cubeo\n",
      "Abau\n",
      "Uzbek\n",
      "Yaminahua\n",
      "Abui\n",
      "Warembori\n",
      "Japanese\n",
      "Nivkh\n",
      "Kham\n",
      "Aleut\n",
      "Seri\n",
      "Koasati\n",
      "Cashibo\n",
      "Washo\n",
      "Tuvan\n",
      "Massachusett\n",
      "Santali\n",
      "Kwaza\n",
      "Palikur\n",
      "Even\n",
      "Klamath\n",
      "Ordos\n",
      "Kayapó\n",
      "Kamasau\n",
      "Tutelo\n",
      "Karajá\n",
      "Mikasuki\n",
      "Dagur\n",
      "Cholón\n",
      "Cocama\n",
      "Kadiwéu\n",
      "Bisu\n",
      "Apinayé\n",
      "Itelmen\n",
      "Guaraní\n",
      "Lavukaleve\n",
      "Awtuw\n",
      "Golin\n",
      "Lepcha\n",
      "Savosavo\n",
      "Mian\n",
      "Ainu\n",
      "Edolo\n",
      "Tshangla\n",
      "Tonkawa\n",
      "Kusunda\n",
      "German\n",
      "Cupeño\n",
      "Bangime\n",
      "Betoi\n",
      "Manchu\n",
      "Bao'an\n",
      "Finnish\n",
      "Guajajara\n",
      "Turkish\n",
      "Nevome\n",
      "Tlingit\n",
      "Hungarian\n",
      "Oneida\n",
      "Ute\n",
      "Wappo\n",
      "Cavineña\n",
      "Chipewyan\n"
     ]
    }
   ],
   "source": [
    "for i in lang_set:\n",
    "    if i in targets:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = Extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(langs):\n",
    "    data = dict()\n",
    "    data[\"language\"] = []\n",
    "    data[\"extracted\"] = []\n",
    "    data[\"target\"] = []\n",
    "    for lang in langs:\n",
    "        #try:\n",
    "            if lang not in data[\"language\"]:\n",
    "                target = extractor.extract(lang, \"Word order\", \"BM25\")\n",
    "                text = \"\"\n",
    "                for item in target:\n",
    "                    text += item\n",
    "                    text += \"\\n\\n-------------------------------\\n\\n\"\n",
    "                data[\"language\"].append(lang)\n",
    "                data[\"extracted\"].append(text)\n",
    "                data[\"target\"].append(targets[lang])\n",
    "        #except:\n",
    "            #pass\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>extracted</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Khanty</td>\n",
       "      <td>As to the order of the elements of the constr...</td>\n",
       "      <td>SOV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Khmer</td>\n",
       "      <td>Should you order the food?\\n\\n\\n-------------...</td>\n",
       "      <td>SVO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Guajajara</td>\n",
       "      <td>Aqui estão alguns exemplos de orações e períod...</td>\n",
       "      <td>VSO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chukchi</td>\n",
       "      <td>PRAGMATICS OF SENTENCE FORMChapter 19\\nelement...</td>\n",
       "      <td>No dominant order</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    language                                          extracted  \\\n",
       "0     Khanty   As to the order of the elements of the constr...   \n",
       "1      Khmer   Should you order the food?\\n\\n\\n-------------...   \n",
       "2  Guajajara  Aqui estão alguns exemplos de orações e períod...   \n",
       "3    Chukchi  PRAGMATICS OF SENTENCE FORMChapter 19\\nelement...   \n",
       "\n",
       "              target  \n",
       "0                SOV  \n",
       "1                SVO  \n",
       "2                VSO  \n",
       "3  No dominant order  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = get_df([\"Khanty\", \"Khmer\", \"Guajajara\", \"Chukchi\"])\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>extracted</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basque</td>\n",
       "      <td>In (1c), the sentence contains a transitive ve...</td>\n",
       "      <td>SOV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Burushaski</td>\n",
       "      <td>Brsk. lacks the rich derivational machinery ch...</td>\n",
       "      <td>SOV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seneca</td>\n",
       "      <td>145\\nA Grammar of the Seneca Language\\n146\\nA ...</td>\n",
       "      <td>No dominant order</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                          extracted  \\\n",
       "0      Basque  In (1c), the sentence contains a transitive ve...   \n",
       "1  Burushaski  Brsk. lacks the rich derivational machinery ch...   \n",
       "2      Seneca  145\\nA Grammar of the Seneca Language\\n146\\nA ...   \n",
       "\n",
       "              target  \n",
       "0                SOV  \n",
       "1                SOV  \n",
       "2  No dominant order  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val = get_df([\"Basque\", \"Burushaski\", \"Seneca\"])\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "labels = {\"SOV\": 0,\n",
    "          \"SVO\": 1,\n",
    "          \"VSO\": 2,\n",
    "          \"No dominant order\": 3\n",
    "          }\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = [labels[label] for label in df[\"target\"]]\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df[\"extracted\"]]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.804                 | Train Accuracy:  0.250                 | Val Loss:  1.064                 | Val Accuracy:  0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.802                 | Train Accuracy:  0.000                 | Val Loss:  0.968                 | Val Accuracy:  1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.793                 | Train Accuracy:  0.250                 | Val Loss:  1.035                 | Val Accuracy:  0.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.835                 | Train Accuracy:  0.000                 | Val Loss:  1.002                 | Val Accuracy:  0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.789                 | Train Accuracy:  0.250                 | Val Loss:  1.052                 | Val Accuracy:  0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "                  \n",
    "EPOCHS = 5\n",
    "model = BertClassifier()\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature 24A: Locus of Marking in Possessive Noun Phrases\n",
    "\n",
    "Feature 23A: Locus of Marking in the Clause\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4614c025658c69f22720a6288eae577516a08c27018e2e14910e16f5ccdcf612"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
